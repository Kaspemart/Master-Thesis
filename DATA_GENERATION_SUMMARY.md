# Data Generation Summary

**Date:** January 18, 2026  
**Status:** âœ… **COMPLETE**

---

## Overview

Successfully generated synthetic training data for neural network-based parameter estimation of the basic stochastic volatility model.

---

## Generated Datasets

### Dataset Specifications

| Dataset | Sequences | File Size | Location |
|---------|-----------|-----------|----------|
| Training | 50,000 | 185.21 MB | `data/synthetic/basic_sv/train.npz` |
| Validation | 10,000 | 37.04 MB | `data/synthetic/basic_sv/val.npz` |
| Test | 10,000 | 37.04 MB | `data/synthetic/basic_sv/test.npz` |
| **Total** | **70,000** | **259.29 MB** | - |

### Data Structure

Each dataset contains:
```python
{
    'returns': np.ndarray,      # Shape: (N, 252) - observed returns
    'parameters': np.ndarray,   # Shape: (N, 2) - [phi, sigma_v]
    'log_volatility': np.ndarray  # Shape: (N, 252) - latent states
}
```

---

## Configuration

### Model Parameters
- **Sequence length (T):** 252 (one trading year of daily data)
- **Initial log-volatility (hâ‚€):** 0.0
- **Random seed:** 42 (reproducible)

### Parameter Sampling Ranges
- **Ï† (persistence):** Uniform[0.85, 0.98]
  - Controls volatility clustering
  - Higher values â†’ stronger persistence
  
- **Ïƒáµ¥ (vol-of-vol):** Uniform[0.10, 0.40]
  - Controls volatility variability
  - Higher values â†’ more jumpy volatility

---

## Data Quality Verification

### Training Set Statistics
```
Returns:  mean=0.0002, std=1.1666
Ï†:        mean=0.9151, std=0.0375, range=[0.85, 0.98]
Ïƒ_v:      mean=0.2501, std=0.0865, range=[0.10, 0.40]
```

### Validation Set Statistics
```
Returns:  mean=0.0003, std=1.1643
Ï†:        mean=0.9154, std=0.0374, range=[0.85, 0.98]
Ïƒ_v:      mean=0.2497, std=0.0868, range=[0.10, 0.40]
```

### Test Set Statistics
```
Returns:  mean=0.0003, std=1.1676
Ï†:        mean=0.9148, std=0.0375, range=[0.85, 0.98]
Ïƒ_v:      mean=0.2494, std=0.0863, range=[0.10, 0.40]
```

### Quality Checks âœ…
- âœ… Returns have mean â‰ˆ 0 (correct for zero-drift model)
- âœ… Parameters uniformly distributed across specified ranges
- âœ… All three splits have similar statistics (no distribution shift)
- âœ… Full coverage of parameter space
- âœ… No outliers or anomalies detected

---

## Usage Example

### Loading Data

```python
from src.data.synthetic.basic_sv import BasicSVDataGenerator

# Load training data
train_data = BasicSVDataGenerator.load_dataset('data/synthetic/basic_sv/train.npz')

# Access components
returns = train_data['returns']        # Shape: (50000, 252)
parameters = train_data['parameters']  # Shape: (50000, 2)

# Extract individual parameters
phi = parameters[:, 0]      # Persistence
sigma_v = parameters[:, 1]  # Vol-of-vol

# Use in neural network training
X_train = returns           # Input: return sequences
y_train = parameters        # Output: [phi, sigma_v]
```

### Regenerating Data

If you need to regenerate or create additional datasets:

```python
from src.data.synthetic.basic_sv import BasicSVDataGenerator

# Create generator
generator = BasicSVDataGenerator(
    phi_range=(0.85, 0.98),
    sigma_v_range=(0.10, 0.40),
    T=252,
    seed=42
)

# Generate new data
data = generator.generate(n_sequences=10000)

# Or generate splits at once
train, val, test = generator.generate_splits(
    n_train=50000,
    n_val=10000,
    n_test=10000
)
```

---

## Performance

- **Generation time:** ~15 seconds for 70,000 sequences
- **Generation rate:** ~4,700 sequences/second
- **Storage format:** NumPy compressed `.npz` (efficient and fast)

---

## Next Steps

### Immediate (Next 1-2 weeks)
1. **Implement LSTM estimator** (`src/neural_networks/lstm_estimator.py`)
   - Architecture: LSTM layers + dense output
   - Input: return sequences (252 timesteps)
   - Output: 2 parameters [Ï†, Ïƒáµ¥]

2. **Data preprocessing utilities**
   - Normalization/standardization of returns
   - Parameter normalization (if needed)
   - PyTorch DataLoader setup

3. **Training pipeline**
   - Loss function: MSE on parameters
   - Optimizer: Adam
   - Learning rate scheduling
   - Early stopping on validation loss

### Medium-term (2-4 weeks)
4. **Model training and evaluation**
   - Train LSTM on 50k sequences
   - Hyperparameter tuning
   - Evaluate on test set

5. **Classical benchmark** (MLE or particle filter)
   - Implement one classical method
   - Run on test set for comparison

6. **Comparison and analysis**
   - Accuracy comparison (MSE, MAE, bias)
   - Computational speed comparison
   - Robustness analysis

---

## Files Created

### Implementation
- `src/data/synthetic/basic_sv/generator.py` - Main data generator class
- `src/data/synthetic/basic_sv/__init__.py` - Module exports
- `src/data/synthetic/__init__.py` - Package init
- `src/data/__init__.py` - Package init

### Generated Data
- `data/synthetic/basic_sv/train.npz` - Training set
- `data/synthetic/basic_sv/val.npz` - Validation set
- `data/synthetic/basic_sv/test.npz` - Test set

---

## Key Decisions Made

1. **Sequence length T=252:** One trading year is standard in literature and provides enough information for parameter estimation

2. **Parameter ranges:** Based on empirical studies in financial econometrics
   - High persistence (Ï† > 0.85) typical for financial data
   - Moderate vol-of-vol (Ïƒáµ¥ < 0.4) prevents unrealistic volatility jumps

3. **Uniform sampling:** Ensures good coverage of parameter space for neural network training

4. **Large training set (50k):** Neural networks need substantial data; this is a safe starting point

5. **Separate val/test splits:** Validation for hyperparameter tuning, test for final evaluation (never seen during development)

---

## Technical Notes

### Why These Parameter Ranges?

**Ï† âˆˆ [0.85, 0.98]:**
- Literature shows daily financial returns have high volatility persistence
- Values below 0.85 are unrealistic (volatility too mean-reverting)
- Values above 0.98 approach non-stationarity
- Kim, Shephard & Chib (1998) typically estimate Ï† â‰ˆ 0.95-0.98

**Ïƒáµ¥ âˆˆ [0.10, 0.40]:**
- Controls how variable the volatility itself is
- Too low (< 0.1): volatility barely changes (unrealistic)
- Too high (> 0.4): volatility too jumpy (rare in real data)
- Typical estimates in literature: 0.15-0.30

### Storage Format

Using `.npz` (NumPy compressed) because:
- âœ… Fast loading (native NumPy format)
- âœ… Good compression (~40% smaller than uncompressed)
- âœ… Simple to use (no external dependencies)
- âœ… Preserves exact numerical precision

Alternative formats considered but rejected:
- âŒ CSV: Too slow, large files, precision loss
- âŒ HDF5: Overkill for this size, extra dependency
- âŒ Pickle: Less portable, version-dependent

---

## Troubleshooting

### If you need more data
```python
generator = BasicSVDataGenerator(seed=43)  # Different seed
extra_data = generator.generate(n_sequences=10000)
generator.save_dataset(extra_data, 'data/synthetic/basic_sv/extra.npz')
```

### If you need different parameter ranges
```python
generator = BasicSVDataGenerator(
    phi_range=(0.90, 0.99),  # Focus on very high persistence
    sigma_v_range=(0.15, 0.25),  # Narrower vol-of-vol range
    seed=42
)
```

### If you need longer sequences
```python
generator = BasicSVDataGenerator(T=500, seed=42)  # Two years
```

---

## References

- Kim, S., Shephard, N., & Chib, S. (1998). Stochastic volatility: likelihood inference and comparison with ARCH models. *The Review of Economic Studies*, 65(3), 361-393.

---

**Status:** Ready for neural network training! ğŸš€

